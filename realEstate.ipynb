{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports (Pandas - Python Data Analysis Library, Numbpy - Array Proessing Package, Seaborn - Statistical Data Visualisation, Matplotlib - Static Vizualization Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Import Structure for EDA;\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing Requests for use in API calls.\n",
    "import requests\n",
    "\n",
    "\n",
    "# Import Standard Packages for Date and Time;\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import Beautiful Soup, Selenium, for Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Import packages to help with parsing Information\n",
    "import lxml\n",
    "from lxml.html.soupparser import fromstring\n",
    "import prettify\n",
    "import numbers\n",
    "import htmltext\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Expensive   Affordable\n",
      "0            Elon         Eden\n",
      "1           Boone        Ayden\n",
      "2         Woodfin       Maiden\n",
      "3        Carrboro       Hamlet\n",
      "4       Asheville       Newton\n",
      "5       Pineville     Sawmills\n",
      "6      Wilmington     ArchDale\n",
      "7   Morehead City   Rockingham\n",
      "8  Black Mountain  Winterville\n",
      "9  Hendersonville  Gibsonville\n"
     ]
    }
   ],
   "source": [
    "# Set Maximum Displayed Columns to None; All Columns Listed\n",
    "pd.options.display.max_columns = None;\n",
    "\n",
    "# Set MatPlotLib to Display Elements\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Cities in CSV file for 10 most and least expenive north carolina cities.csv\n",
    "cities_df = pd.read_csv(\"cities.csv\",header=0)\n",
    "\n",
    "print(cities_df)\n",
    "\n",
    "# Initialize Variables to be Read into.\n",
    "address = ''\n",
    "price = 0\n",
    "beds = 0\n",
    "details = 0\n",
    "home_type = ''\n",
    "last_updated = ''\n",
    "brokerage = ''\n",
    "link = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon\n",
      "Eden\n",
      "Boone\n",
      "Ayden\n",
      "Woodfin\n",
      "Maiden\n",
      "Carrboro\n",
      "Hamlet\n",
      "Asheville\n",
      "Newton\n",
      "Pineville\n",
      "Sawmills\n",
      "Wilmington\n",
      "ArchDale\n",
      "Morehead City\n",
      "Rockingham\n",
      "Black Mountain\n",
      "Winterville\n"
     ]
    }
   ],
   "source": [
    "# Insert Headers for various websites to pull information specifically.\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Checking for Sale using Zillow.com\n",
    "# Using 20 Cities we use 20 URLs\n",
    "# Using loop to collect information from csv to pass to beautiful soup\n",
    "for i in range(9):\n",
    "    for j in range(2):\n",
    "        # Get City from cities_df\n",
    "        city = cities_df.iat[i,j]\n",
    "        print(city)\n",
    "\n",
    "        # Desired City\n",
    "        location = city0+',-NC_rb/'\n",
    "\n",
    "        # URL coresponding to Zillow.com\n",
    "        url = 'https://www.zillow.com/homes/for_sale/'+location    \n",
    "        # Use Get Request\n",
    "        r = requests.get(url, headers=req_headers)\n",
    "\n",
    "        soup = BeautifulSoup(r0.text, 'html.parser')\n",
    "\n",
    "        house_df = pd.DataFrame()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
